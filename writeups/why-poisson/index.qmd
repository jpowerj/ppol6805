---
title: "Why Poisson Processes are 'The' Core Model for Testing Spatial Hypotheses"
author: "Jeff Jacobs"
institute: "`jj1088@georgetown.edu`"
date: 2025-10-16
sidebar: mainnav
bibliography: "../../_PPOL6805.bib"
csl: "../../chicago-17-no-url.csl"
code-fold: show
categories:
  - Extra Writeups
assignments:
  - All
format:
  html:
    df-print: kable
    link-external-icon: true
    link-external-newwindow: true
---

## Overview

I fumbled this concept towards the end of the Week 8 lecture, since I was rushing to try and get to **intensity functions** and **pair correlation functions**. As we know, this kind of guilt will weigh on Jeff's mind until he creates a writeup where he can slow down and walk through it with the proper attention to detail... so here we are!

The reason we worry so much about the "proper" null model to use when we want to test **spatial** hypotheses (and the reason for creating a full-on Writeup about it), is that certain intuitions we may have developed in our standard (non-spatial) statistics courses fail to **generalize** easily to our current two-dimensional **spatial** setting! So, let's:

1. Start by looking at what may be in our heads from these earlier classes when we hear the term "null model" ([Part 1](#part-1-the-null-model-from-introductory-statistics)), then
2. Describe the trouble we run into if we try to haphazardly take the one-dimensional statistical notion of **independent and identically distributed (i.i.d.) random variables** and just "lift" it into two dimensions ([Part 2](#part-2-naÃ¯vely-generalizing-to-the-spatial-statistical-setting)),
3. Start more slowly, building up from the simplest possible point process to see where exactly this naÃ¯ve approach goes wrong ([Part 3](#part-3-what-kind(s-of-spatial-randomness-do-we-want-to-model)), and finally
4. Conclude with the less-rushed version of what I said in class: that the **Poisson Point Process** model "saves us" by avoiding these troubles ([Part 4](#part-4-saved-by-the-poisson-distribution))

## Part 1: The Null Model from Introductory Statistics

In **standard** "one-dimensional" statistics, when we start building up our conceptual/methodological toolbox towards the goal of developing a set of statistical **hypothesis tests**, we usually start with the notion of a collection $X_1, X_2, \ldots, X_n$ of **$N$ independent and identically distributed (i.i.d.) Random Variables**.

The reason this odd term "independent and identically distributed" is so important is because is exactly this **i.i.d.** property that underlies the two key theorems of statistical sampling theory: the **Law of Large Numbers (LLN)** and the **Central Limit Theorem (CLT)**.

Say we have a goal of figuring out the **average height** of Georgetown students, in centimeters. We don't have any way to instantaneously "divine" this information from out of nowhere, however, so instead we decide to start **sampling** students from campus. We let $X_1$ be the Random Variable corresponding to the height of the first person we sample, $X_2$ be the Random Variable corresponding to the height of the second person we sample, and so on, up to $X_n$, the Random Variable corresponding to the height of the final person we sample.

* We make the assumption that the Random Variables in this series $\{X_1, X_2, \ldots, X_n\}$ are **independent**, since we believe that learning the height of any one person doesn't give us any information about the height of other people.
* We make the assumption that the Random Variables in this series $\{X_1, X_2, \ldots, X_n\}$ are **identically distributed** because we believe there is some underlying distribution (perhaps a Normal distribution) describing the **population** of Georgetown students, and that each of our samples is a single **draw** from this (assumed) underlying population distribution.

The second assumption (that $\{X_1, X_2, \ldots, X_n\}$ are **identically distributed**) is super important for the wording of the LLN and CLT. To see exactly why these theorems are useful, we'll add one more piece of information to this assumption, namely, that the underlying population distribution we've assumed has some well-defined **mean** $\mu$^[In slightly more detail, in the context we've set up, this assumption of a "well-defined" mean $\mu$ is saying that $\mathbb{E}[X_i] = \mu$, i.e., that the most likely value, the specific numeric value we most "expect" to see when $X_i$ is realized, is the value $\mu$.]. We actually don't need to assume anything more than this, but for the sake of making a picture let's also assume a well-defined standard deviation $\sigma$.

These assumptions now allow us to visualize the **population distribution** of height, the distribution of the heights of all possible Georgetown students in the past, present, and future:

```{r}
#| label: pop-dist
#| code-fold: true
library(tidyverse) |> suppressPackageStartupMessages()
library(latex2exp) |> suppressPackageStartupMessages()
set.seed(6807)
mu_label <- TeX("$\\mu$")
cb_palette <- c(
  "#E69F00", "#56B4E9", "#009E73", "#F0E442",
  "#0072B2", "#D55E00", "#CC79A7"
)
height_dist <- function(x) dnorm(x, mean = 170, sd = 10)
xlim_df <- tibble(x=c(135, 205))
base_pop_plot <- xlim_df |> ggplot(aes(x=x)) +
  geom_area(
    stat = "function",
    fun = height_dist,
    color = "black",
    fill = cb_palette[1],
    alpha = 0.75
  ) +
  theme_classic(base_size=16) +
  theme(plot.title = element_text(hjust=0.5)) +
  labs(
    y="Density"
  )
base_pop_plot +
  scale_x_continuous(
    breaks=seq(from=140, to=200, by=10),
    labels=c("Î¼-3Ïƒ","Î¼-2Ïƒ","Î¼-Ïƒ","Î¼","Î¼+Ïƒ","Î¼+2Ïƒ","Î¼+3Ïƒ")
  ) +
  labs(
    title="Hypothesized Population Distribution: N(Î¼,Ïƒ)"
  )
```

We won't have any way of knowing the actual **values** $\mu$ and $\sigma$ in practice (if we did, we wouldn't need to use this whoe sampling thing in the first place), but to make our visualization a little more concrete, let's assume that this abstract population mean height is $\mu = 170\text{cm}$ and population mean SD is $\sigma = 10\text{cm}$. Then the above visualization becomes:

```{r}
#| label: pop-dist-values
#| code-fold: true
(pop_plot <- base_pop_plot +
  labs(
    title="Hypothesized Population Distribution: N(170, 10)"
  )
)
```

If our goal is to estimate this now-assumed underling $\mu$ value, we can frame the LLN and CLT for our sample of size $N$, $\{X_1, X_2, \ldots, X_n\}$ as follows. Consider the **mean** of these i.i.d. RVs,
$$
\overline{X}_{N} = \frac{1}{N}(X_1 + X_2 + \cdots + X_n).
$$

Notice how this RV $\overline{X}_N$ is _itself_ a Random Variable, with some distribution! We can actually use R again to visualize this **separate** distribution, of the number we get when we compute the **mean** of $N$ samples:

```{r}
#| label: sampling-dist
sample_from_pop <- function(N = 5) {
  return(rnorm(n=N, mean=170, sd=10))
}
for (i in 1:3) {
  cur_sample <- sample_from_pop()
  sample_str <- paste0(round(cur_sample, 2), collapse=", ")
  writeLines(paste0("Sample #",i,": ",sample_str))
  cur_sample_mean <- mean(cur_sample)
  writeLines(paste0("Mean = ",round(cur_sample_mean,2)))
}
```

And if we repeat this 999 times, we'll get 999 different sample means: that is, 999 different **realized values** of $\overline{X}_N$ (here we display only the first 100 of these realized values, for brevity):

```{r}
#| label: many-sample-means
sample_means <- replicate(999, mean(sample_from_pop()))
sample_means[1:100]
```

If we plot the distribution of **these** values, we get a different distribution from the above. This time, it's a **sampling** distribution! It's a distribution of the **averages** we get when we **sample** from the population distribution:

```{r}
#| label: sampling-dist-histogram
#| code-fold: true
sample_mean_df <- tibble(x=sample_means)
sample_plot <- sample_mean_df |> ggplot(aes(x=x)) +
  geom_histogram(
    binwidth=0.5,
    fill=cb_palette[2],
    alpha=0.75
  ) +
  theme_classic(base_size=15) +
  labs(title="Distribution of 999 Means from Samples of Size N = 5") +
  theme(plot.title = element_text(hjust=0.5))
sample_plot
```

By overlaying this **sampling** distribution on top of the **population** distribution (though remember, again, that in the real world we're never going to be able to discover this "true" population distribution), we can start to see what's going on:

```{r}
#| label: pop-and-sampling-dist
#| warning: false
#| code-fold: true
ggplot() +
  geom_area(
    data = xlim_df,
    mapping = aes(x=x),
    stat = "function",
    fun = height_dist,
    color = "black",
    fill = cb_palette[1],
    alpha = 0.75
  ) +
  geom_histogram(
    data = sample_mean_df,
    mapping = aes(x=x),
    stat = "density",
    binwidth = 0.5,
    fill = cb_palette[2],
    alpha = 0.75
  ) +
  theme_classic(base_size=15) +
  labs(title="Sampling (Blue) vs. Population (Orange) Dists (N = 5)")
```

The values we're getting for the **sample mean**, in blue here, **vary** around the **true mean** that we're trying to estimate. And, intuitively, if we have the resources to take **bigger** samples (e.g., more time to ask more than $N = 5$ people), this should give us even **more accurate** estimates of the population mean, which is what we indeed find in our simulation:

```{r}
#| label: increasing-n
#| warning: false
#| code-fold: true
sample_sizes <- c(5, 30, 100, 500)
sample_means_df <- tibble(N = numeric(), xbar=numeric())
for (cur_sample_size in sample_sizes) {
  cur_sample_means <- replicate(
    1000,
    sample_from_pop(cur_sample_size) |> mean()
  )
  cur_sm_df <- tibble(
    N = cur_sample_size,
    xbar=cur_sample_means
  )
  sample_means_df <- bind_rows(sample_means_df, cur_sm_df)
}
sample_means_df |> head()
sample_means_df |> ggplot() +
  geom_area(
    data = xlim_df,
    mapping = aes(x=x),
    stat = "function",
    fun = height_dist,
    color = "black",
    fill = cb_palette[1],
    alpha = 0.75
  ) +
  geom_histogram(
    data = sample_means_df,
    mapping = aes(x=xbar),
    stat = "density",
    binwidth = 0.5,
    alpha = 0.5,
    fill=cb_palette[2]
  ) +
  facet_wrap(vars(N), nrow=2, ncol=2, scales="free_y") +
  theme_classic() +
  labs(title="Sampling (Blue) vs. Population (Orange) Distributions for Increasing N") +
  theme(plot.title = element_text(hjust=0.5))
```

The two key theorems of sampling theory therefore **describe** (along with formal mathematical proofs) what we can see intuitively from the four plots above: that is, what happens to this RV as $N$ gets bigger and bigger (goes towards $\infty$):

* The **Law of Large Numbers** tells us that the **middle** of the blue sampling distribution will converge to the **population mean** $\mu$.
  * It says in essence that we can pick any really small value $\varepsilon$, and then, regardless of what particular small value we pick, the probability that $\overline{X}$ is more than $\varepsilon$ away from the (assumed) "true" mean value $\mu$ goes to 0 as we take more and more samples: $\Pr\left(|\overline{X} - \mu| > \varepsilon\right) \rightarrow 0$ as $N \rightarrow \infty$.
* The **Central Limit Theorem** gives us more specifics about the **shape** of the blue sampling distribution, and about **how quickly** the convergence guaranteed by the LLN will proceed.
  * It says that the blue sampling distribution will specifically be a **Normal** distribution, whose mean will be $\mu$ and whose standard deviation will be $\sigma / \sqrt{N}$. In symbols: $\overline{X}_{N} \sim \mathcal{N}\left(\mu, \frac{\sigma}{\sqrt{N}}\right)$. The presence of $N$ in the standard deviation is why we see the blue sampling distribution getting thinner and thinner above: as we raise $N$, $\sigma$ stays the same but $\sqrt{N}$ gets bigger and bigger, which makes the overall standard deviation term get smaller and smaller!

Having built up to these two key theorems, we can now perform statistical hypothesis testing! Because, we can:

* Take any **specific** sample we happen to have, $\{X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n\}$, and
* Compare it to the **expected shape of the sampling distribution** under some **hypothesis**!

Continuing our above example, if we think that the "true" mean height of Georgetown students is $\mu = 170\text{cm}$, we can take a sample of $N$ Georgetown students and compare the **mean height value** of **this particular sample** with the **distribution of mean height values** that a **population distribution with $\mu = 170\text{cm}$ would generate**.

For example, if we take a sample of $N = 30$ Georgetown students, and find that the mean height of these 30 students is **160cm**, we can "test" how likely it is that this particular mean height value would arise relative to our **working hypothesis** that the "true" population mean is $\mu = 170\text{cm}$... just take the blue distribution from the $N = 30$ plot above, and check **where 160cm falls** relative to this distribution! Here's what this looks like:

```{r}
#| label: hypothesis-test
#| warning: false
#| code-fold: true
n30_df <- sample_means_df |>
  filter(N == 30)
n30_df |> ggplot() +
  geom_histogram(
    mapping = aes(x = xbar),
    fill = cb_palette[2],
    binwidth = 0.5
  ) +
  geom_vline(
    xintercept = 160,
    linetype = "dashed",
    linewidth = 0.75,
    color = cb_palette[3]
  ) +
  theme_classic(base_size = 15) +
  labs(title="Observed Mean (160cm) vs. 1000 Simulated Means") +
  theme(plot.title = element_text(hjust=0.5))
```

The vertical (dashed) green line in this plot is the **mean we actually obtained** from our sample of 30 students. The blue histogram represents **all of the means** we got when we simulated taking a sample of size $N = 30$ from the **hypothesized** population distribution $\mathcal{N}(170, 10)$... and what it tells us is that the mean we actually observed is **not very likely to arise** from our hypothesized population... Meaning, in turn, that we **decrease our confidence** in our hypothesis, relative to whatever our confidence was before we collected this sample!

## Part 2: "NaÃ¯vely" Generalizing to the Spatial-Statistical Setting

The next idea is that, if we want to statistically test hypotheses about observed **spatial** distributions, we'd like an **equivalent to the above normal distribution** for the **two-dimensional spatial settings** (Random Fields) we analyze in this course!

In the same way that we just tested our **observed mean** of $160\text{cm}$ against the range of means we would **expect** to see under some hypothesis, we'd like to test **observed point patterns** against some range of **expected point patterns** under some hypothesis. In this case, rather than a hypothesis about the **mean** $\mu$, let's start with a hypothesis about the first **spatial statistic** we learned in Week 7, namely, Moran's $I$! Let's say we think there is a propensity for points to cluster together very tightly, so that although the specific cluster patterns may vary slightly from year to year, we think that the underlying **population** autocorrelation $I$ is close to $1.0$.

The reason for "overthinking" the Poisson process at this point is, we're going to need to think about how we might **falsify** this hypothesis^[This is not specific to spatial statistics, or even statistics in general -- it's the foundation of the so-called "Popperian" scientific method, which Jeff needs to not rant about here ðŸ˜œ].

Like we did above for the simpler "one-dimensional" hypothesis about the population mean height, we'll subject our hypothesis $\mathcal{H} = [I \approx 1]$ to **falsification testing** by developing a "null model": a model of what a world where our hypothesis is **wrong** would look like.

With this "null model" in hand, we can then simulate what **not-high** spatial autocorrelation would look like within the same spatial domain. In fact, since the null model is a **generative** model, we can use it to generate as many worlds-where-my-hypothesis-is-false as we'd like! Above we arbitrarily chose to stop after we had generated 999 such worlds, and we'll do the same for most examples in this course.

Once we have these 999 simulated null-world patterns, we can compute a Moran's $I$ value for each, producing $\{I_1, I_2, \ldots, I_{999}\}$, which we call the "null distribution" of our "test statistic" $I$. We then **compare** $I_{\text{obs}}$ with these 999 simulated $I$ values:

* If we find that the observed autocorrelation value $I_{\text{obs}}$, calculated with respect to the point pattern we **actually saw**, is sufficiently **different** from the range of $I$ values $\{I_1, I_2, \ldots, I_{999}$} that we calculated from the **simulated** patterns, we should^[The word "should" might feel weird here, as a normative/prescriptive term suddenly appearing in the middle of a descriptive statistical procedure -- in Bayesian statistics, though, we actually have a good reason for using "should": any other rule we might use for evaluating hypotheses is provably "sub-optimal", at least in the narrow sense that we'll make worse predictions in the future. This is true, sadly, even for the "standard" hypothesis test setup you may have learned, where you "reject" or "fail to reject" the null hypothesis: we can do better in terms of future predictions by converting "reject" into "decrease my belief by an amount determined by Bayes' Rule" and "fail to reject" into "increase my belief by an amount determined by Bayes' Rule"! You can find proofs and applications and etc. in @jaynes_probability_2002] **increase our confidence** in the idea that the **underlying** spatial pattern is indeed one with high autocorrelation (relative to our confidence before carrying out this test)
* If we find that the observed autocorrelation value $I_{\text{obs}}$, calculated with respect to the point pattern we **actually saw**, is **not** very different from the range of $I$ values $\{I_1, I_2, \ldots, I_{999}$} that we calculated from the **simulated** patterns, we should **decrease our confidence** in the idea that the **underlying** spatial pattern is indeed one with high autocorrelation (relative to our confidence before carrying out this test)

So, if we observed (say) $N_{\text{obs}} = 60$ points in some region, and we're hypothesizing that the **underlying** micro-level process generating these points is one with the property of **high autocorrelation**... our instinct may very reasonably be: "Cool! Let's go generate 999 point patterns, each with $N = 60$ points spread randomly across the same region!"

There is, unfortunately, a big **red flag** inherent in this approach that we need to train our brains to notice. The issue with this approach, that makes it **inappropriate** as a null model for most spatial data analyses, is **extremely subtle** but **extremely important** as we move forward with our spatial data analysis unit! It boils down to **where exactly we think the "randomness" is happening in the "random spatial process"**.

To move from the above intuitive-but-deficient approach to the notion of **Complete Spatial Randomness (CSR)** that is the "standard" null model for spatial data, let's follow Sections 5.2 and 5.3 (pages 128 to 137) of @baddeley_spatial_2015 by moving slowly, starting from the simplest possible point process.

## Part 3: What Kind(s) of Spatial Randomness Do We Want to Model?

### Model 1: Uniform Point Process

To start, consider the simplest possible point process we can imagine: just taking some 2D shape like a (unit) square and placing a single point randomly within it. If we place our unit square in the first quadrant of the Cartesian plane, then to generate a random point $\mathbf{s}_0$ within this square we literally just need to generate a random $x$ coordinate $X \sim \mathcal{U}[0,1]$ and a random $y$ coordinate $Y \sim \mathcal{U}[0,1]$, as in the following plot:

```{r}
#| label: random-point
#| code-fold: true
library(latex2exp)
set.seed(6808)
x_coords <- runif(n = 4, min = 0, max = 1) |>
  round(2)
y_coords <- runif(n = 4, min = 0, max = 1) |>
  round(2)
rand_point_df <- tibble::tibble(
  index=1:4,
  x=x_coords,
  y=y_coords
)
rand_point_df <- rand_point_df |> mutate(
  label_tex = paste0(
    '$s_',
    index,
    '$ = (',
    x,
    ", ",
    y,
    ")"
  )
)
rand_point_df
appender <- function(s) {
  return(TeX(s))
}
tex_labeller = as_labeller(
  appender,
  default = label_parsed
) 

base_point_plot <- rand_point_df |>
  ggplot(aes(x=x, y=y)) +
  # xlim(0, 1) + ylim(0, 1) +
  expand_limits(
    x=c(-0.05, 1.05),
    y=c(-0.05, 1.05)
  ) +
  scale_x_continuous(
    breaks=c(0, 0.5, 1.0),
    labels=c(0, 0.5, 1.0)
  ) +
  scale_y_continuous(
    breaks=c(0, 0.5, 1.0),
    labels=c(0, 0.5, 1.0)
  ) +
  theme_classic(base_size=15) +
  coord_equal() +
  theme(plot.title=element_text(hjust=0.5))
base_point_plot +
  geom_point(size=3) +
  geom_segment(
    mapping = aes(
      yend=-Inf
    ),
    linewidth=0.75,
    linetype='dashed'
  ) +
  geom_segment(
    mapping = aes(
      xend=-Inf
    ),
    linewidth=0.75,
    linetype='dashed'
  ) +
  facet_wrap(
    vars(label_tex), nrow=2, ncol=2,
    labeller=tex_labeller,
    # scales='free_x'
  )

```

Now, if we take this and try to generalize it into a probabilistic model, we should at minimum be able to ask a question like, what's the probability of getting **this** particular point (or any other point in the unit square, but, we have a specific example above we can work with for concreteness). From [previous probability and statistics courses](https://jjacobs.me/dsan5100/w04/slides.html#/discrete-vs.-continuous) you might know that the answer is... zero! When working in **continuous spaces** like the unit square, we actually **can't compute probabilities for individual, infinitesimally-small points**.

So, given this limitation, when we are working with probabilities in continuous spaces we employ a simple but powerful "trick" that ends up unlocking the whole world of **continuous probability distributions**: we shift away from thinking about probabilities of **points** and instead think about probabilities of **areas**: unlike the question

> "What is the probability that a randomly-generated point $\mathbf{s}$ in $D$ appears at the **exact coordinates** $(x,y) \in D$?",

which always has the answer 0, we instead ask

> "What is the probability that a randomly-generated point $\mathbf{s}$ in $D$ appears within some **area** $A \subseteq D$?",

which has a well-formed, meaningful answer! An answer that we can therefore use, for example, to identify **subregions** of $D$ with higher and lower **densities** of points (the thing we've been hoping to do all along, since these subregions represent areas with **clusters** of points!)

For example, the following plot illustrates how we can meaningfully ask the question

> "What is the probability that a randomly-generated point in $D$ appears in the *right half* of $D$?",

to obtain the answer $\frac{1}{2}$, as the **proportion** of the area covered by $A$ to the **total area** of our working domain $D$ (in this case, the total area of the unit square, which is 1).

```{r}
#| label: area-probabilities
#| code-fold: true
#| warning: false


# library(latex2exp)
# area_label <- TeX('$A \\subseteq D$')
# area_prob_label <- TeX('$\\Pr(s \\in A) = \\frac{1}{2}$')
# point_label <- TeX('$s_0$')
# point_prob_label <- TeX('$\\Pr(s = s_0) = 0$')
# base_point_plot +
#   geom_rect(
#     xmin=0.5, xmax=1.0,
#     ymin=0.0, ymax=1.0,
#     fill=cb_palette[1],
#     alpha=0.5
#   ) +
#   geom_point(size=3) +
#   geom_segment(
#     mapping = aes(
#       yend=-Inf
#     ),
#     linewidth=0.75,
#     linetype='dashed'
#   ) +
#   geom_segment(
#     mapping = aes(
#       xend=-Inf
#     ),
#     linewidth=0.75,
#     linetype='dashed'
#   ) +
#   geom_text(
#     x=0.75, y=0.9,
#     label=area_label,
#     size=8
#   ) +
#   geom_text(
#     x=0.75, y=0.7,
#     label=area_prob_label,
#     size=6
#   ) +
#   geom_text(
#     x = 0.75, y=0.3,
#     label=point_prob_label,
#     size=6,
#   ) +
#   geom_text(
#     label=point_label,
#     size=6,
#     nudge_y = 0.05
#   )
```

This notion, that we can only meaningfully ask about the probability of a point **falling within some area**, is key to why the next model (which is in fact the "intuitive" model from earlier) fails as a workable null model for spatial data science.

### Model 2: Binomial Point Process

Now, if we wanted to generate **more than one** point, our next guess might be to start by **choosing a number of points $N$**, and then **generating coordinates for each of the $N$ points uniformly** within $D$... Does that sound familiar? That is exactly the "intuitive" null model we came up with above, as a way of testing the "randomness" (as opposed to "clustered-ness" or "regular-ness") of some observed collection of points!

A collection of points generated with this method looks as follows

## Part 4: Saved by the Poisson Distribution!

### Model 3: Poisson Point Process

## References

::: {#refs}
:::
